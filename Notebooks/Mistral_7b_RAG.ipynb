{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDSd0WtHzKSG"
      },
      "outputs": [],
      "source": [
        "! pip install -q accelerate bitsandbytes safetensors langchain chromadb  datasets sentence-transformers\n",
        "! pip install git+https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import DataFrameLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from datasets import load_dataset\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import  pipeline\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import transformers\n",
        "from torch import cuda\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "W0v5t6gtpVHW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "model_name = \"bn22/Mistral-7B-Instruct-v0.1-sharded\"\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "embed_model = HuggingFaceEmbeddings(\n",
        "    model_name=embed_model_id,\n",
        "    model_kwargs={'device': device},\n",
        "    encode_kwargs={'device': device, 'batch_size': 32}\n",
        ")"
      ],
      "metadata": {
        "id": "oaqODnhXoOtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_4bit=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto'\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.bos_token_id = 1\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=2048,\n",
        "    temperature=0,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.2\n",
        ")\n",
        "\n",
        "local_llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "wiPOm4Tezfku"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = load_dataset(\"ayoubkirouane/arxiv-math\"  , split=\"train\")\n",
        "train_dataset = train_dataset.to_pandas()\n",
        "train_dataset['text'] = train_dataset[\"question\"] +  train_dataset[\"answer\"]\n",
        "df_document = DataFrameLoader(train_dataset[:1000] , page_content_column=\"text\").load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=256, chunk_overlap=10)\n",
        "texts = text_splitter.split_documents(df_document)"
      ],
      "metadata": {
        "id": "JFldvwJqm9Qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chromadb_index = Chroma.from_documents(texts, embed_model , persist_directory=\"DB\")"
      ],
      "metadata": {
        "id": "Gtn50Oi-rPkG"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_llm(\"How do neutrino fluxes are determined?\t\")"
      ],
      "metadata": {
        "id": "654xIND1207j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'How do neutrino fluxes are determined?\t'\n",
        "\n",
        "chromadb_index.similarity_search(\n",
        "    query,  # the search query\n",
        "    k=2  # returns top 3 most relevant chunks of text\n",
        ")"
      ],
      "metadata": {
        "id": "92L3FzTS3AUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_qa = RetrievalQA.from_chain_type(\n",
        "    llm=local_llm, chain_type=\"stuff\", retriever=chromadb_index.as_retriever(search_kwargs={\"k\": 5})\n",
        ")"
      ],
      "metadata": {
        "id": "SSaW9-RCz3g4"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = document_qa.run(\"How do neutrino fluxes are determined?\t\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "GfAHYbX7z9Rh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}